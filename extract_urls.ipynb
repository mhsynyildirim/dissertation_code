{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all urls\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Initialize an empty set to store unique URLs\n",
    "checked_urls = set()\n",
    "\n",
    "# Function to extract URLs from a webpage\n",
    "def extract_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        full_url = urljoin(url, link['href'])\n",
    "        # Check if the URL starts with 'https://www.aru.ac.uk'\n",
    "        if full_url.startswith('https://www.aru.ac.uk'):\n",
    "            yield full_url\n",
    "\n",
    "# Start the process with the base URL\n",
    "urls_to_check = list(extract_links(\"https://www.aru.ac.uk/\"))\n",
    "\n",
    "# Open the output file\n",
    "with open('urls.txt', 'w', encoding='utf-8') as file:\n",
    "    # Loop until there are no more URLs to check\n",
    "    while urls_to_check:\n",
    "        url = urls_to_check.pop(0)\n",
    "        if url not in checked_urls:\n",
    "            checked_urls.add(url)\n",
    "            print(f\"'{url}'\")  # Print the URL\n",
    "            file.write(f\"{url}\\n\")  # Write the URL to the file\n",
    "            try:\n",
    "                # Only add new URLs if they haven't been checked yet and are not already in the list to check\n",
    "                new_urls = [u for u in extract_links(url) if u not in checked_urls and u not in urls_to_check]\n",
    "                urls_to_check.extend(new_urls)\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Print the total number of URLs\n",
    "print(f\"Total URLs: {len(checked_urls)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed URLs: 204\n",
      "Remained URLs: 44279\n"
     ]
    }
   ],
   "source": [
    "#Filter Urls\n",
    "# Open the existing file\n",
    "with open('urls.txt', 'r', encoding='utf-8') as file:\n",
    "    all_urls = [line.strip() for line in file]\n",
    "\n",
    "# Define a set of unwanted elements in the URLs\n",
    "unwanted_in_urls = {'email-protection', '404-not-found', 'blogs', 'jpg', 'JPG', 'doc'}\n",
    "\n",
    "# Filter out the URLs\n",
    "filtered_urls = [\n",
    "    url for url in all_urls \n",
    "    if url.startswith('https://www.aru.ac.uk') and not any(element in url for element in unwanted_in_urls)\n",
    "]\n",
    "\n",
    "# Count the number of removed and remained URLs\n",
    "removed_count = len(all_urls) - len(filtered_urls)\n",
    "remained_count = len(filtered_urls)\n",
    "print(f\"Removed URLs: {removed_count}\")\n",
    "print(f\"Remained URLs: {remained_count}\")\n",
    "\n",
    "# Write the remaining URLs back into the file\n",
    "with open('url.txt', 'w', encoding='utf-8') as file:\n",
    "    for url in filtered_urls:\n",
    "        file.write(f\"{url}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
